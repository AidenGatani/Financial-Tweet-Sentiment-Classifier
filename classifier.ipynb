{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "691770dc-9388-4727-a3ed-7dfa9e795290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc48c1-5ec7-4abf-8659-25cd3c477103",
   "metadata": {},
   "source": [
    "# Loading The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05854a4c-a053-4d90-bf64-8faecbe7681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"zeroshot/twitter-financial-news-topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d875e4bd-8cd5-4166-8efb-ee654350bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "label_mapping = {\n",
    "    0: \"Analyst Update\",\n",
    "    1: \"Fed | Central Banks\",\n",
    "    2: \"Company | Product News\",\n",
    "    3: \"Treasuries | Corporate Debt\",\n",
    "    4: \"Dividend\",\n",
    "    5: \"Earnings\",\n",
    "    6: \"Energy | Oil\",\n",
    "    7: \"Financials\",\n",
    "    8: \"Currencies\",\n",
    "    9: \"General News | Opinion\",\n",
    "    10: \"Gold | Metals | Materials\",\n",
    "    11: \"IPO\",\n",
    "    12: \"Legal | Regulation\",\n",
    "    13: \"M&A | Investments\",\n",
    "    14: \"Macro\",\n",
    "    15: \"Markets\",\n",
    "    16: \"Politics\",\n",
    "    17: \"Personnel Change\",\n",
    "    18: \"Stock Commentary\",\n",
    "    19: \"Stock Movement\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5baa6e-64a0-4cf1-9255-d4cf50cb4db2",
   "metadata": {},
   "source": [
    "## Exploring The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2969cd-04c6-42eb-98d8-cb7a644409ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16990\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 4117\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ca0d5-a70b-467f-a56c-ed7ceb797c6f",
   "metadata": {},
   "source": [
    "# Loading The Model And Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657a3aed-4d7b-4b5d-b252-dd8822b215f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5929aa33-cfa3-496c-aaab-222baad74726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize The data\n",
    "tokenized_datasets = {}\n",
    "\n",
    "for split in dataset.keys():\n",
    "    tokenized_datasets[split] = dataset[split].map(lambda x: tokenizer(x['text'], truncation=True, padding=\"max_length\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f5b82c-f89d-455a-88d5-ce1cd42d210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 16990\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 4117\n",
       " })}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2009ab25-376d-4509-95a5-e4359e8cdfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load The Model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=20,\n",
    "                                 id2label=label_mapping)\n",
    "\n",
    "# freeze Model Parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef8a50-1fb0-413f-a991-a55492c2e543",
   "metadata": {},
   "source": [
    "## Exploring The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3891a0-2666-46ad-91fc-d0ebf351f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b69c210-859a-4da7-a9a2-ae48ee4ded3c",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae3e9bf0-a87d-4f46-ba4d-85995d110438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e66fc3e9-53b6-4248-98cb-b0304bfdd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fb90387-0400-4760-ab7b-a189327239ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd5446eb-8f19-49b7-a641-26df3d323f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "\n",
    "def trainer(model, tokenizer, datasets, compute_metrics):\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./data',\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        load_best_model_at_end=True,\n",
    "        learning_rate=2e-5,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch')\n",
    "    \n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b545f09-9ad0-4c48-89f9-b6795997f952",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LoraConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create Lora Config\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mLoraConfig\u001b[49m(r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, target_modules\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_lin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk_lin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv_lin\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      3\u001b[0m                     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m                     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mSEQ_CLS)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LoraConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Lora Config File\n",
    "config = LoraConfig(r=10, target_modules=['q_lin', 'k_lin', 'v_lin'], \n",
    "                    lora_alpha=16, lora_dropout=0.1, bias=\"none\", \n",
    "                    task_type=TaskType.SEQ_CLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5f66fb6-20ab-4f98-be8d-7899910a3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model With PEFT Config File\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a797d9e6-0644-438d-af9a-c3c45f0360ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 882,452 || all params: 67,851,304 || trainable%: 1.3005674879881455\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cc483-359f-49b3-b5ac-f1301274997f",
   "metadata": {},
   "source": [
    "## Evaluate Model Prior To FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2ae4261-6297-45c6-9416-a3e794f14ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainer(model, tokenizer, tokenized_datasets, compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09d7a9-f531-4157-8e25-bd22d10efa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809fd4f-8ed4-43a2-982a-663fb70bc19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='371' max='4117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 371/4117 02:35 < 26:13, 2.38 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0b26d-42c5-43e3-9d33-e286b41fe200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tokenized_datasets[\"validation\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "predictions = peft_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa14a3-d407-4f7c-a058-ab2ea5e60938",
   "metadata": {},
   "source": [
    "## Train PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22925028-d70a-4f22-9904-4b507962d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_trainer = trainer(lora_model, tokenizer, tokenized_datasets, compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d958a3-365f-4ae8-bea5-706f03d901de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='1062' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/1062 01:57 < 4:55:20, 0.06 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef4574-6eba-406c-8e2a-b7ce94b33eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT Model Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1f99b-3c9a-40bb-9957-f1b97f799a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2dd50-a8ed-408c-b88c-6ef33ff6f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tokenized_datasets[\"validation\"])\n",
    "df = df[[\"text\", \"label\"]]\n",
    "predictions = peft_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
